{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":53569,"databundleVersionId":5834979,"sourceType":"competition"}],"dockerImageVersionId":30497,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Introduction**","metadata":{}},{"cell_type":"markdown","source":"The aim of this project is to predict the sentiment of the movies from the given dataset. In this Kaggle notebook I have implements the projects aim in my best of knowledge of ML models and its fucntionalities. I have made **11 Machine Learning models** and compare them with each other for a more comprehensive understanding and to predict the sentiments of the movies in a effective and efficent manner.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder,OneHotEncoder,MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.feature_extraction import text\nimport scipy.sparse as sp\nfrom sklearn.decomposition import TruncatedSVD\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"id":"S0FoZtz_SFcp","execution":{"iopub.status.busy":"2023-08-23T20:49:58.815904Z","iopub.execute_input":"2023-08-23T20:49:58.816292Z","iopub.status.idle":"2023-08-23T20:50:00.872809Z","shell.execute_reply.started":"2023-08-23T20:49:58.816260Z","shell.execute_reply":"2023-08-23T20:50:00.871653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Loading**","metadata":{"id":"92H0R5nRSFcw"}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/sentiment-prediction-on-movie-reviews/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/sentiment-prediction-on-movie-reviews/test.csv\")\nmovies = pd.read_csv(\"/kaggle/input/sentiment-prediction-on-movie-reviews/movies.csv\")","metadata":{"id":"le8zM9tF2c_i","execution":{"iopub.status.busy":"2023-08-23T20:50:00.875171Z","iopub.execute_input":"2023-08-23T20:50:00.875530Z","iopub.status.idle":"2023-08-23T20:50:03.218324Z","shell.execute_reply.started":"2023-08-23T20:50:00.875474Z","shell.execute_reply":"2023-08-23T20:50:03.217065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:03.220162Z","iopub.execute_input":"2023-08-23T20:50:03.220649Z","iopub.status.idle":"2023-08-23T20:50:03.231272Z","shell.execute_reply.started":"2023-08-23T20:50:03.220604Z","shell.execute_reply":"2023-08-23T20:50:03.229843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:03.234687Z","iopub.execute_input":"2023-08-23T20:50:03.235120Z","iopub.status.idle":"2023-08-23T20:50:03.244555Z","shell.execute_reply.started":"2023-08-23T20:50:03.235081Z","shell.execute_reply":"2023-08-23T20:50:03.243134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movies.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:03.246749Z","iopub.execute_input":"2023-08-23T20:50:03.247165Z","iopub.status.idle":"2023-08-23T20:50:03.256848Z","shell.execute_reply.started":"2023-08-23T20:50:03.247129Z","shell.execute_reply":"2023-08-23T20:50:03.255567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:03.258869Z","iopub.execute_input":"2023-08-23T20:50:03.259258Z","iopub.status.idle":"2023-08-23T20:50:03.504772Z","shell.execute_reply.started":"2023-08-23T20:50:03.259222Z","shell.execute_reply":"2023-08-23T20:50:03.503587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:03.506089Z","iopub.execute_input":"2023-08-23T20:50:03.506412Z","iopub.status.idle":"2023-08-23T20:50:03.572337Z","shell.execute_reply.started":"2023-08-23T20:50:03.506385Z","shell.execute_reply":"2023-08-23T20:50:03.571459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movies.info()","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:03.573596Z","iopub.execute_input":"2023-08-23T20:50:03.574139Z","iopub.status.idle":"2023-08-23T20:50:04.074205Z","shell.execute_reply.started":"2023-08-23T20:50:03.574109Z","shell.execute_reply":"2023-08-23T20:50:04.072980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:04.076418Z","iopub.execute_input":"2023-08-23T20:50:04.077388Z","iopub.status.idle":"2023-08-23T20:50:04.105702Z","shell.execute_reply.started":"2023-08-23T20:50:04.077345Z","shell.execute_reply":"2023-08-23T20:50:04.104571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:04.110251Z","iopub.execute_input":"2023-08-23T20:50:04.110630Z","iopub.status.idle":"2023-08-23T20:50:04.121753Z","shell.execute_reply.started":"2023-08-23T20:50:04.110601Z","shell.execute_reply":"2023-08-23T20:50:04.120668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movies.head(3)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:04.123483Z","iopub.execute_input":"2023-08-23T20:50:04.124050Z","iopub.status.idle":"2023-08-23T20:50:04.147107Z","shell.execute_reply.started":"2023-08-23T20:50:04.124020Z","shell.execute_reply":"2023-08-23T20:50:04.145989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:04.148339Z","iopub.execute_input":"2023-08-23T20:50:04.148780Z","iopub.status.idle":"2023-08-23T20:50:04.370902Z","shell.execute_reply.started":"2023-08-23T20:50:04.148741Z","shell.execute_reply":"2023-08-23T20:50:04.369693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:04.372358Z","iopub.execute_input":"2023-08-23T20:50:04.372833Z","iopub.status.idle":"2023-08-23T20:50:04.496667Z","shell.execute_reply.started":"2023-08-23T20:50:04.372794Z","shell.execute_reply":"2023-08-23T20:50:04.495241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movies.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:04.503672Z","iopub.execute_input":"2023-08-23T20:50:04.508196Z","iopub.status.idle":"2023-08-23T20:50:05.008410Z","shell.execute_reply.started":"2023-08-23T20:50:04.508133Z","shell.execute_reply":"2023-08-23T20:50:05.007157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Preprocessing**","metadata":{"id":"Zdld7QNLSFc8"}},{"cell_type":"markdown","source":"* **Data Set merging and duplicate droping**","metadata":{"id":"K4bo5WlqSFc-"}},{"cell_type":"code","source":"df = movies.drop_duplicates(subset=['movieid'], keep='first')\nmerged_df = pd.merge(train, df, on='movieid', how='left')\nmerged_df1 = pd.merge(test, df, on='movieid', how='left')","metadata":{"id":"KY6c0oJS2c_n","execution":{"iopub.status.busy":"2023-08-23T20:50:05.010033Z","iopub.execute_input":"2023-08-23T20:50:05.010765Z","iopub.status.idle":"2023-08-23T20:50:05.521531Z","shell.execute_reply.started":"2023-08-23T20:50:05.010718Z","shell.execute_reply":"2023-08-23T20:50:05.520351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:05.522782Z","iopub.execute_input":"2023-08-23T20:50:05.523105Z","iopub.status.idle":"2023-08-23T20:50:05.530751Z","shell.execute_reply.started":"2023-08-23T20:50:05.523076Z","shell.execute_reply":"2023-08-23T20:50:05.529606Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:05.532141Z","iopub.execute_input":"2023-08-23T20:50:05.532775Z","iopub.status.idle":"2023-08-23T20:50:05.544699Z","shell.execute_reply.started":"2023-08-23T20:50:05.532746Z","shell.execute_reply":"2023-08-23T20:50:05.543524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df1.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:05.546420Z","iopub.execute_input":"2023-08-23T20:50:05.546844Z","iopub.status.idle":"2023-08-23T20:50:05.556293Z","shell.execute_reply.started":"2023-08-23T20:50:05.546815Z","shell.execute_reply":"2023-08-23T20:50:05.555192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:05.558092Z","iopub.execute_input":"2023-08-23T20:50:05.558396Z","iopub.status.idle":"2023-08-23T20:50:06.402886Z","shell.execute_reply.started":"2023-08-23T20:50:05.558370Z","shell.execute_reply":"2023-08-23T20:50:06.401715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df1.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:06.404387Z","iopub.execute_input":"2023-08-23T20:50:06.404876Z","iopub.status.idle":"2023-08-23T20:50:06.658839Z","shell.execute_reply.started":"2023-08-23T20:50:06.404839Z","shell.execute_reply":"2023-08-23T20:50:06.657663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Changing Column name**","metadata":{"id":"yidey8MpSFdD"}},{"cell_type":"code","source":"new_column_name={'isTopCritic': 'isFrequentReviewer'}\nmerged_df1=merged_df1.rename(columns=new_column_name)","metadata":{"id":"XDnVUyo-2c_r","execution":{"iopub.status.busy":"2023-08-23T20:50:06.660300Z","iopub.execute_input":"2023-08-23T20:50:06.660739Z","iopub.status.idle":"2023-08-23T20:50:06.697503Z","shell.execute_reply.started":"2023-08-23T20:50:06.660696Z","shell.execute_reply":"2023-08-23T20:50:06.696326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df1.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:06.699334Z","iopub.execute_input":"2023-08-23T20:50:06.699790Z","iopub.status.idle":"2023-08-23T20:50:06.721782Z","shell.execute_reply.started":"2023-08-23T20:50:06.699741Z","shell.execute_reply":"2023-08-23T20:50:06.720392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Target column encoding**","metadata":{"id":"cVckgBfHSFdV"}},{"cell_type":"code","source":"merged_df[\"sentiment\"] = merged_df[\"sentiment\"].replace(np.nan, 'Positive')\ny=merged_df[\"sentiment\"]\nencoder = OrdinalEncoder()\ny_enc = encoder.fit_transform(y.to_numpy().reshape(-1, 1))\n#Remove the sentiment label column from the merged data\nmerged_df = merged_df.drop('sentiment', axis=1)\ny_enc","metadata":{"id":"as-iNpX62c_u","outputId":"518bf5c8-233b-45b2-acee-c16097892597","execution":{"iopub.status.busy":"2023-08-23T20:50:06.723870Z","iopub.execute_input":"2023-08-23T20:50:06.724285Z","iopub.status.idle":"2023-08-23T20:50:06.866431Z","shell.execute_reply.started":"2023-08-23T20:50:06.724246Z","shell.execute_reply":"2023-08-23T20:50:06.865255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_enc.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:06.868380Z","iopub.execute_input":"2023-08-23T20:50:06.869199Z","iopub.status.idle":"2023-08-23T20:50:06.876316Z","shell.execute_reply.started":"2023-08-23T20:50:06.869157Z","shell.execute_reply":"2023-08-23T20:50:06.875133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:06.878214Z","iopub.execute_input":"2023-08-23T20:50:06.880532Z","iopub.status.idle":"2023-08-23T20:50:06.889936Z","shell.execute_reply.started":"2023-08-23T20:50:06.880470Z","shell.execute_reply":"2023-08-23T20:50:06.888462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:06.891675Z","iopub.execute_input":"2023-08-23T20:50:06.893572Z","iopub.status.idle":"2023-08-23T20:50:06.915238Z","shell.execute_reply.started":"2023-08-23T20:50:06.893527Z","shell.execute_reply":"2023-08-23T20:50:06.913772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Removing unwanted symbols**","metadata":{}},{"cell_type":"code","source":"import re\nmerged_df1['genre'] = merged_df1['genre'].str.replace(re.compile(\" & \"), \",\")\nmerged_df['genre'] = merged_df['genre'].str.replace(re.compile(\" & \"), \",\")\nmerged_df['ratingContents'] = merged_df['ratingContents'].apply(lambda x: re.sub(r\"\\[|\\]\", \"\", str(x))) # convert x to a string before replacing brackets\nmerged_df1['ratingContents'] = merged_df1['ratingContents'].apply(lambda x: re.sub(r\"\\[|\\]\", \"\", str(x))) # convert x to a string before replacing brackets","metadata":{"id":"hhWE1goJSFd2","execution":{"iopub.status.busy":"2023-08-23T20:50:06.918381Z","iopub.execute_input":"2023-08-23T20:50:06.919523Z","iopub.status.idle":"2023-08-23T20:50:07.864539Z","shell.execute_reply.started":"2023-08-23T20:50:06.919476Z","shell.execute_reply":"2023-08-23T20:50:07.863420Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Splitting Categorical and numerical columns**","metadata":{}},{"cell_type":"code","source":"categorical = merged_df.select_dtypes(include=['object', 'bool'])\ncategorical_drop = categorical.drop(['reviewText','ratingContents','genre','director'],axis=1)\ncategorical_names = categorical_drop.columns.tolist()\nremaining_categorical_names=['reviewText','ratingContents','genre','director']\n\nnumerical = merged_df.select_dtypes(include=['float64'])\nnumerical_names = numerical.columns.tolist()\n\n# Print the column names\nprint(\"Categorical columns:\", categorical_names)\nprint(\"Categorical remaining columns :\", remaining_categorical_names)\nprint(\"Numerical columns:\", numerical_names)","metadata":{"id":"trUtNImT2c_0","outputId":"dff3cce5-27e7-459e-9a4e-16f9705144a1","execution":{"iopub.status.busy":"2023-08-23T20:50:07.877418Z","iopub.execute_input":"2023-08-23T20:50:07.877807Z","iopub.status.idle":"2023-08-23T20:50:08.036717Z","shell.execute_reply.started":"2023-08-23T20:50:07.877778Z","shell.execute_reply":"2023-08-23T20:50:08.035530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Imputing missing values using replace function**","metadata":{}},{"cell_type":"code","source":"merged_df['genre'] = merged_df['genre'].replace(np.nan, 'Drama')\nmerged_df1['genre'] = merged_df1['genre'].replace(np.nan, 'Drama')\nmerged_df['ratingContents'] = merged_df['ratingContents'].replace('nan',np.nan)\nmerged_df1['ratingContents'] = merged_df1['ratingContents'].replace('nan', np.nan)\nmerged_df['ratingContents'] = merged_df['ratingContents'].replace(np.nan,'Thematic Material')\nmerged_df1['ratingContents'] = merged_df1['ratingContents'].replace(np.nan, 'Thematic Material')\nmerged_df['director'] = merged_df['director'].replace(np.nan, 'Information not available')\nmerged_df1['director'] = merged_df1['director'].replace(np.nan, 'Information not available')\n","metadata":{"id":"wGsbK_kBSFd-","execution":{"iopub.status.busy":"2023-08-23T20:50:08.038092Z","iopub.execute_input":"2023-08-23T20:50:08.038530Z","iopub.status.idle":"2023-08-23T20:50:08.177653Z","shell.execute_reply.started":"2023-08-23T20:50:08.038479Z","shell.execute_reply":"2023-08-23T20:50:08.176402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Categorical and numerical transformer combined pipeline\ncategory_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')\n  )])\n\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', MinMaxScaler())\n])\n\ntransformer_pipeline = ColumnTransformer(transformers=[\n    ('cat', category_transformer, categorical_names ),\n    ('num', numerical_transformer,  numerical_names)\n], remainder='passthrough')\n\n# Main pipeline\npipeline = Pipeline([('preprocessor', transformer_pipeline)])\n\n# Fit and transform the train data\ntransformed_df = pd.DataFrame(pipeline.fit_transform(merged_df), columns=categorical_names  + numerical_names + remaining_categorical_names)\ntransformed_df[numerical_names] = transformed_df[numerical_names].astype(float)\nmissing_in_train_values = transformed_df.isna().sum()\nprint(\"\\nTrain data:\\n\")\nprint(missing_in_train_values)\n\n# transform the test data\ntransformed_df1 = pd.DataFrame(pipeline.transform(merged_df1), columns=categorical_names  + numerical_names + remaining_categorical_names)\ntransformed_df1[numerical_names] = transformed_df1[numerical_names].astype(float)\nmissing_in_test_values = transformed_df1.isna().sum()\nprint(\"\\nTest data:\\n\")\nprint( missing_in_test_values)","metadata":{"id":"yddlPWGR2c_1","outputId":"29ce451f-790e-4e89-aff7-d959e9db73e3","execution":{"iopub.status.busy":"2023-08-23T20:50:08.179428Z","iopub.execute_input":"2023-08-23T20:50:08.179764Z","iopub.status.idle":"2023-08-23T20:50:10.365036Z","shell.execute_reply.started":"2023-08-23T20:50:08.179736Z","shell.execute_reply":"2023-08-23T20:50:10.363938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# transformed_df['reviewText']= transformed_df['reviewText'].fillna(\" \")\n# transformed_df1['reviewText']= transformed_df1['reviewText'].fillna(\" \")","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:10.366319Z","iopub.execute_input":"2023-08-23T20:50:10.366667Z","iopub.status.idle":"2023-08-23T20:50:10.371002Z","shell.execute_reply.started":"2023-08-23T20:50:10.366638Z","shell.execute_reply":"2023-08-23T20:50:10.369970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"relevant_columns = ['title', 'director','genre','ratingContents']\n# For train data\nsubset = transformed_df[transformed_df['reviewText'].isnull()]\nfor index, row in subset.iterrows():\n    review_text = ' '.join(str(row[column]) for column in relevant_columns if pd.notnull(row[column]))\n    transformed_df.at[index, 'reviewText'] = review_text.strip()\n\n# For test data\nsubset1 = transformed_df1[transformed_df1['reviewText'].isnull()]\nfor index, row in subset1.iterrows():\n    review_text1 = ' '.join(str(row[column]) for column in relevant_columns if pd.notnull(row[column]))\n    transformed_df1.at[index, 'reviewText'] = review_text1.strip()","metadata":{"id":"A8Kyh3jX2c_6","execution":{"iopub.status.busy":"2023-08-23T20:50:10.372433Z","iopub.execute_input":"2023-08-23T20:50:10.373467Z","iopub.status.idle":"2023-08-23T20:50:11.592287Z","shell.execute_reply.started":"2023-08-23T20:50:10.373429Z","shell.execute_reply":"2023-08-23T20:50:11.591348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **Using TF-IDF Vectorizer**","metadata":{"id":"t_-1AydwSFf5"}},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(binary=False,ngram_range=(1,3),max_features=25)\nvectorized = vectorizer.fit_transform(merged_df['genre'])\narray = vectorized.toarray()\nprint(\"vectorized output array of genre:\")\nprint(array)\n\nvectorized1 = vectorizer.fit_transform(merged_df1['genre'])\narray1 = vectorized1.toarray()\nprint(\"vectorized output array1 of genre :\")\nprint(array1)","metadata":{"id":"t7KybSmJSFeB","outputId":"9b3cdf89-2ef7-4f20-8922-9b9e4d8950d5","execution":{"iopub.status.busy":"2023-08-23T20:50:11.593486Z","iopub.execute_input":"2023-08-23T20:50:11.593861Z","iopub.status.idle":"2023-08-23T20:50:14.235177Z","shell.execute_reply.started":"2023-08-23T20:50:11.593831Z","shell.execute_reply":"2023-08-23T20:50:14.233912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rc_vectorizer = TfidfVectorizer(binary=False,ngram_range=(1,3),max_features=30)\nrc_vectorized = rc_vectorizer.fit_transform(merged_df['ratingContents'])\nrc_array = rc_vectorized.toarray()\nprint(\"vectorized output (train):\\n\")\nprint(rc_array)\n\nrc_vectorized1 = rc_vectorizer.fit_transform(merged_df1['ratingContents'])\nrc_array1 = rc_vectorized1.toarray()\nprint(\"\\nvectorized output (test):\\n\")\nprint(rc_array1)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:14.236813Z","iopub.execute_input":"2023-08-23T20:50:14.237515Z","iopub.status.idle":"2023-08-23T20:50:18.544975Z","shell.execute_reply.started":"2023-08-23T20:50:14.237461Z","shell.execute_reply":"2023-08-23T20:50:18.543567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = text.ENGLISH_STOP_WORDS  # English stop words\ntfidf_vectorizer = TfidfVectorizer( stop_words='english',binary=False,ngram_range=(1,3),max_features=3000)\n\n# TF-IDF on train dataset\ntfidf_vectorized = tfidf_vectorizer.fit_transform(transformed_df['reviewText'])\ntfidf_array = tfidf_vectorized.toarray()\n\n# TF-IDF on test dataset\ntfidf_vectorized1 = tfidf_vectorizer.transform(transformed_df1['reviewText'])\ntfidf_array1 = tfidf_vectorized1.toarray()\n\nprint(\"TF-IDF vectorized (train) output:\\n\")\nprint(tfidf_array)\nprint(\"\\nTF-IDF vectorized (test) output:\\n\")\nprint(tfidf_array)","metadata":{"id":"pZgOV3dz2c_7","outputId":"a26ec112-4abf-49c4-db32-6f94e131bde0","execution":{"iopub.status.busy":"2023-08-23T20:50:18.546703Z","iopub.execute_input":"2023-08-23T20:50:18.547163Z","iopub.status.idle":"2023-08-23T20:50:52.500465Z","shell.execute_reply.started":"2023-08-23T20:50:18.547123Z","shell.execute_reply":"2023-08-23T20:50:52.499352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer_releaseDateTheaters = TfidfVectorizer()\nvectorized_releaseDateTheaters = vectorizer_releaseDateTheaters.fit_transform(transformed_df['releaseDateTheaters'])\narray_releaseDateTheaters = vectorized_releaseDateTheaters.toarray()\nprint(\"Vectorized releaseDateTheaters array:\")\nprint(array_releaseDateTheaters)\n\nvectorized1_releaseDateTheaters = vectorizer_releaseDateTheaters.transform(transformed_df1['releaseDateTheaters'])\narray1_releaseDateTheaters = vectorized1_releaseDateTheaters.toarray()\nprint(\"Vectorized releaseDateTheaters array 1:\")\nprint(array1_releaseDateTheaters)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:52.502220Z","iopub.execute_input":"2023-08-23T20:50:52.502983Z","iopub.status.idle":"2023-08-23T20:50:54.653965Z","shell.execute_reply.started":"2023-08-23T20:50:52.502948Z","shell.execute_reply":"2023-08-23T20:50:54.652814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer_releaseDateStreaming = TfidfVectorizer()\nvectorized_releaseDateStreaming = vectorizer_releaseDateStreaming.fit_transform(transformed_df['releaseDateStreaming'])\narray_releaseDateStreaming = vectorized_releaseDateStreaming.toarray()\nprint(\"Vectorized releaseDateStreaming array:\")\nprint(array_releaseDateStreaming)\n\nvectorized1_releaseDateStreaming = vectorizer_releaseDateStreaming.transform(transformed_df1['releaseDateStreaming'])\narray1_releaseDateStreaming = vectorized1_releaseDateStreaming.toarray()\nprint(\"Vectorized releaseDateStreaming array 1:\")\nprint(array1_releaseDateStreaming)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:54.655397Z","iopub.execute_input":"2023-08-23T20:50:54.656347Z","iopub.status.idle":"2023-08-23T20:50:56.583305Z","shell.execute_reply.started":"2023-08-23T20:50:54.656307Z","shell.execute_reply":"2023-08-23T20:50:56.582102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer_rating  = TfidfVectorizer()\nvectorized_rating  = vectorizer_rating.fit_transform(transformed_df['rating'])\narray_rating = vectorized_rating.toarray()\nprint(\"Vectorized rating  array:\")\nprint(array_rating )\n\nvectorized1_rating  = vectorizer_rating.transform(transformed_df1['rating'])\narray1_rating  = vectorized1_rating.toarray()\nprint(\"Vectorized rating  array 1:\")\nprint(array1_rating )\n","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:56.584834Z","iopub.execute_input":"2023-08-23T20:50:56.585190Z","iopub.status.idle":"2023-08-23T20:50:57.700109Z","shell.execute_reply.started":"2023-08-23T20:50:56.585162Z","shell.execute_reply":"2023-08-23T20:50:57.698991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer_originalLanguage = TfidfVectorizer()\nvectorized_originalLanguage = vectorizer_originalLanguage.fit_transform(transformed_df['originalLanguage'])\narray_originalLanguage = vectorized_originalLanguage.toarray()\nprint(\"Vectorized originalLanguage array:\")\nprint(array_originalLanguage)\n\nvectorized1_originalLanguage = vectorizer_originalLanguage.transform(transformed_df1['originalLanguage'])\narray1_originalLanguage = vectorized1_originalLanguage.toarray()\nprint(\"Vectorized originalLanguage array 1:\")\nprint(array1_originalLanguage)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:57.701597Z","iopub.execute_input":"2023-08-23T20:50:57.701964Z","iopub.status.idle":"2023-08-23T20:50:59.210566Z","shell.execute_reply.started":"2023-08-23T20:50:57.701933Z","shell.execute_reply":"2023-08-23T20:50:59.209366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer_director = TfidfVectorizer()\nvectorized_director = vectorizer_director.fit_transform(transformed_df['director'])\narray_director = vectorized_director.toarray()\nprint(\"Vectorized director array:\")\nprint(array_director)\n\nvectorized1_director = vectorizer_director.transform(transformed_df1['director'])\narray1_director = vectorized1_director.toarray()\nprint(\"Vectorized director array 1:\")\nprint(array1_director)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:50:59.212117Z","iopub.execute_input":"2023-08-23T20:50:59.212443Z","iopub.status.idle":"2023-08-23T20:51:03.265367Z","shell.execute_reply.started":"2023-08-23T20:50:59.212415Z","shell.execute_reply":"2023-08-23T20:51:03.264409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer_boxOffice = TfidfVectorizer()\nvectorized_boxOffice = vectorizer_boxOffice.fit_transform(transformed_df['boxOffice'])\narray_boxOffice = vectorized_boxOffice.toarray()\nprint(\"Vectorized boxOffice array:\")\nprint(array_boxOffice)\n\nvectorized1_boxOffice = vectorizer_boxOffice.transform(transformed_df1['boxOffice'])\narray1_boxOffice = vectorized1_boxOffice.toarray()\nprint(\"Vectorized boxOffice array 1:\")\nprint(array1_boxOffice)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:51:03.267072Z","iopub.execute_input":"2023-08-23T20:51:03.267399Z","iopub.status.idle":"2023-08-23T20:51:06.093473Z","shell.execute_reply.started":"2023-08-23T20:51:03.267370Z","shell.execute_reply":"2023-08-23T20:51:06.092318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer_distributor = TfidfVectorizer()\nvectorized_distributor = vectorizer_distributor.fit_transform(transformed_df['distributor'])\narray_distributor = vectorized_distributor.toarray()\nprint(\"Vectorized distributor array:\")\nprint(array_distributor)\n\nvectorized1_distributor = vectorizer_distributor.transform(transformed_df1['distributor'])\narray1_distributor = vectorized1_distributor.toarray()\nprint(\"Vectorized distributor array 1:\")\nprint(array1_distributor)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:51:06.094801Z","iopub.execute_input":"2023-08-23T20:51:06.095124Z","iopub.status.idle":"2023-08-23T20:51:10.131014Z","shell.execute_reply.started":"2023-08-23T20:51:06.095096Z","shell.execute_reply":"2023-08-23T20:51:10.129962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer_soundType = TfidfVectorizer()\nvectorized_soundType = vectorizer_soundType.fit_transform(transformed_df['soundType'])\narray_soundType = vectorized_soundType.toarray()\nprint(\"Vectorized soundType array:\")\nprint(array_soundType)\n\nvectorized1_soundType = vectorizer_soundType.transform(transformed_df1['soundType'])\narray1_soundType = vectorized1_soundType.toarray()\nprint(\"Vectorized soundType array 1:\")\nprint(array1_soundType)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:51:10.132558Z","iopub.execute_input":"2023-08-23T20:51:10.133242Z","iopub.status.idle":"2023-08-23T20:51:11.924936Z","shell.execute_reply.started":"2023-08-23T20:51:10.133201Z","shell.execute_reply":"2023-08-23T20:51:11.923743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# vectorizer_isFrequentReviewer = TfidfVectorizer()\n# vectorized_isFrequentReviewer = vectorizer_isFrequentReviewer.fit_transform(transformed_df['isFrequentReviewer'])\n# array_isFrequentReviewer = vectorized_isFrequentReviewer.toarray()\n# print(\"Vectorized isFrequentReviewer array:\")\n# print(array_isFrequentReviewer)\n\n# vectorized1_isFrequentReviewer = vectorizer_isFrequentReviewer.transform(transformed_df1['isFrequentReviewer'])\n# array1_isFrequentReviewer = vectorized1_isFrequentReviewer.toarray()\n# print(\"Vectorized isFrequentReviewer array 1:\")\n# print(array1_isFrequentReviewer)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:51:11.926583Z","iopub.execute_input":"2023-08-23T20:51:11.927053Z","iopub.status.idle":"2023-08-23T20:51:11.933529Z","shell.execute_reply.started":"2023-08-23T20:51:11.927014Z","shell.execute_reply":"2023-08-23T20:51:11.932184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer_movieid = TfidfVectorizer()\nvectorized_movieid = vectorizer_movieid.fit_transform(transformed_df['movieid'])\narray_movieid = vectorized_movieid.toarray()\nprint(\"Vectorized movieid array:\")\nprint(array_movieid)\n\nvectorized1_movieid = vectorizer_movieid.transform(transformed_df1['movieid'])\narray1_movieid = vectorized1_movieid.toarray()\nprint(\"Vectorized movieid array 1:\")\nprint(array1_movieid)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:51:11.934763Z","iopub.execute_input":"2023-08-23T20:51:11.935214Z","iopub.status.idle":"2023-08-23T20:51:14.887099Z","shell.execute_reply.started":"2023-08-23T20:51:11.935176Z","shell.execute_reply":"2023-08-23T20:51:14.885732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer_title = TfidfVectorizer()\nvectorized_title = vectorizer_title.fit_transform(transformed_df['title'])\narray_title = vectorized_title.toarray()\nprint(\"Vectorized title array:\")\nprint(array_title)\n\nvectorized1_title = vectorizer_title.transform(transformed_df1['title'])\narray1_title = vectorized1_title.toarray()\nprint(\"Vectorized title array 1:\")\nprint(array1_title)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:51:14.891222Z","iopub.execute_input":"2023-08-23T20:51:14.891592Z","iopub.status.idle":"2023-08-23T20:51:17.779474Z","shell.execute_reply.started":"2023-08-23T20:51:14.891561Z","shell.execute_reply":"2023-08-23T20:51:17.778374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer_reviewerName = TfidfVectorizer()\nvectorized_reviewerName = vectorizer_reviewerName.fit_transform(transformed_df['reviewerName'])\narray_reviewerName = vectorized_reviewerName.toarray()\nprint(\"Vectorized reviewerName array:\")\nprint(array_reviewerName)\n\nvectorized1_reviewerName = vectorizer_reviewerName.transform(transformed_df1['reviewerName'])\narray1_reviewerName = vectorized1_reviewerName.toarray()\nprint(\"Vectorized reviewerName array 1:\")\nprint(array1_reviewerName)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:51:17.781561Z","iopub.execute_input":"2023-08-23T20:51:17.781987Z","iopub.status.idle":"2023-08-23T20:51:21.491670Z","shell.execute_reply.started":"2023-08-23T20:51:17.781947Z","shell.execute_reply":"2023-08-23T20:51:21.490521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sparse_matrix = sp.csr_matrix(tfidf_vectorized)\nsparse_matrix1 = sp.csr_matrix(tfidf_vectorized1)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:51:21.493533Z","iopub.execute_input":"2023-08-23T20:51:21.493953Z","iopub.status.idle":"2023-08-23T20:51:21.499236Z","shell.execute_reply.started":"2023-08-23T20:51:21.493915Z","shell.execute_reply":"2023-08-23T20:51:21.498102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sparse = sp.csr_matrix(vectorized)\nsparse1 = sp.csr_matrix(vectorized1)\nrc_sparse = sp.csr_matrix(rc_vectorized)\nrc_sparse1 = sp.csr_matrix(rc_vectorized1)\nrdt_sparse = sp.csr_matrix(vectorized_releaseDateTheaters)\nrdt_sparse1 = sp.csr_matrix(vectorized1_releaseDateTheaters)\nrds_sparse = sp.csr_matrix(vectorized_releaseDateStreaming)\nrds_sparse1 = sp.csr_matrix(vectorized1_releaseDateStreaming)\nol_sparse = sp.csr_matrix(vectorized_originalLanguage)\nol_sparse1 = sp.csr_matrix(vectorized1_originalLanguage)\nd_sparse = sp.csr_matrix(vectorized_director)\nd_sparse1 = sp.csr_matrix(vectorized1_director)\nst_sparse = sp.csr_matrix(vectorized_soundType)\nst_sparse1 = sp.csr_matrix(vectorized_soundType)\nmi_sparse = sp.csr_matrix(vectorized_movieid)\nmi_sparse1 = sp.csr_matrix(vectorized1_movieid)\nt_sparse = sp.csr_matrix(vectorized_title)\nt_sparse1 = sp.csr_matrix(vectorized1_title)\nrn_sparse = sp.csr_matrix(vectorized_reviewerName)\nrn_sparse1 = sp.csr_matrix(vectorized1_reviewerName)\nr_sparse = sp.csr_matrix(vectorized_rating)\nr_sparse1 = sp.csr_matrix(vectorized1_rating)","metadata":{"id":"Ya_u3d8r2dAE","execution":{"iopub.status.busy":"2023-08-23T20:51:21.501092Z","iopub.execute_input":"2023-08-23T20:51:21.501522Z","iopub.status.idle":"2023-08-23T20:51:21.515338Z","shell.execute_reply.started":"2023-08-23T20:51:21.501458Z","shell.execute_reply":"2023-08-23T20:51:21.514231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Visualization after imputing missing values**","metadata":{}},{"cell_type":"markdown","source":"**Data plotting**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport scipy.sparse as sp\n\n# List of sparse matrices and their corresponding names\nsparse_matrices = [\n    (\"reviewText\", sp.csr_matrix(tfidf_vectorized)),\n    (\"genre\", sp.csr_matrix(vectorized)),\n    (\"ratingContents\", sp.csr_matrix(rc_vectorized)),\n    (\"releaseDateTheaters\", sp.csr_matrix(vectorized_releaseDateTheaters)),\n    (\"releaseDateStreaming\", sp.csr_matrix(vectorized_releaseDateStreaming)),\n    (\"originalLanguage\", sp.csr_matrix(vectorized_originalLanguage)),\n    (\"director\", sp.csr_matrix(vectorized_director)),\n    (\"soundType\", sp.csr_matrix(vectorized_soundType)),\n    (\"movieid\", sp.csr_matrix(vectorized_movieid)),\n    (\"title\", sp.csr_matrix(vectorized_title)),\n    (\"reviewerName\", sp.csr_matrix(vectorized_reviewerName)),\n    (\"rating\", sp.csr_matrix(vectorized_rating))\n]\n\n# Filter out sparse matrices with \"vectorized1\" in their names\nfiltered_sparse_matrices = [(name, matrix) for name, matrix in sparse_matrices if \"vectorized1\" not in name]\n\n# Create a correlation matrix\nnum_matrices = len(filtered_sparse_matrices)\ncorrelation_matrix = np.zeros((num_matrices, num_matrices))\n\nfor i in range(num_matrices):\n    for j in range(num_matrices):\n        correlation_matrix[i, j] = (\n            filtered_sparse_matrices[i][1].T.dot(filtered_sparse_matrices[j][1]).sum() /\n            np.sqrt(filtered_sparse_matrices[i][1].T.dot(filtered_sparse_matrices[i][1]).sum() *\n                    filtered_sparse_matrices[j][1].T.dot(filtered_sparse_matrices[j][1]).sum())\n        )\n\n# Extract the names of the remaining sparse matrices\nnames = [name for name, _ in filtered_sparse_matrices]\n\n# Create a heatmap\nplt.figure(figsize=(8,6))\nsns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\",\n            xticklabels=names,\n            yticklabels=names)\nplt.title(\"Correlation Heatmap of Dataset\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:51:21.516980Z","iopub.execute_input":"2023-08-23T20:51:21.517363Z","iopub.status.idle":"2023-08-23T20:51:42.266879Z","shell.execute_reply.started":"2023-08-23T20:51:21.517335Z","shell.execute_reply":"2023-08-23T20:51:42.265701Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Rating is the most non-correalated column as we can in the heat-map.","metadata":{}},{"cell_type":"code","source":"def plot_most_frequent_words(text, title):\n    vectorizer = CountVectorizer(stop_words='english', max_features=50)\n    count_X = vectorizer.fit_transform([text])\n    feature_names = vectorizer.get_feature_names_out()\n    word_frequencies = count_X.toarray()[0]\n    \n    plt.figure(figsize=(16, 10))\n    plt.barh(range(len(feature_names)), word_frequencies, align='center')\n    plt.yticks(range(len(feature_names)), feature_names)\n    plt.xlabel('Frequency')\n    plt.ylabel('Words')\n    plt.title(title)\n    plt.gca().invert_yaxis()\n    plt.show()\n\n\n# Negative reviews\nreviews = [str(transformed_df['reviewText'][i]) for i in range(len(transformed_df['reviewText']))]\ntext = ' '.join(reviews)\nplot_most_frequent_words(text, 'Top 50 Most Frequent Words in Reviews')\n","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:51:42.268127Z","iopub.execute_input":"2023-08-23T20:51:42.268463Z","iopub.status.idle":"2023-08-23T20:51:49.444869Z","shell.execute_reply.started":"2023-08-23T20:51:42.268435Z","shell.execute_reply":"2023-08-23T20:51:49.443728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The word 'film' is the most frequent word in the review text","metadata":{}},{"cell_type":"code","source":"#Plotting frequency of movies released by top 50 reviewerName\n\n# reviewerName data\nreviewerName = transformed_df['reviewerName']\n\n# Count the occurrences of each reviewerName\nreviewerName_counts = reviewerName.value_counts()\n\n# Select the top 50 reviewerName\ntop_50 = reviewerName_counts.head(50)\n\n# Plotting\nplt.figure(figsize=(14, 10))\ntop_50.plot(kind='bar', color='#90EE90')\nplt.title('Top 50 Reviewer by Number of Movies')\nplt.xlabel('Reviewer Name')\nplt.ylabel('Number of Movies')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:51:49.446404Z","iopub.execute_input":"2023-08-23T20:51:49.447610Z","iopub.status.idle":"2023-08-23T20:51:50.407070Z","shell.execute_reply.started":"2023-08-23T20:51:49.447570Z","shell.execute_reply":"2023-08-23T20:51:50.405948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sherri Morrison is the most frequent reviewer as we can see from the bar graph","metadata":{}},{"cell_type":"code","source":"#Plotting frequency of movies released by top 50 director \n\n# director data\ndirector = transformed_df['director']\n\n# Count the occurrences of each director \ndirector_counts = director.value_counts()\n\n# Select the top 50 director \ntop_50 = director_counts.head(50)\n\n# Plotting\nplt.figure(figsize=(14, 10))\ntop_50.plot(kind='bar', color='#90EE90')\nplt.title('Top 50 Director by Number of Movies')\nplt.xlabel('Director Name')\nplt.ylabel('Number of Movies')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:51:50.408792Z","iopub.execute_input":"2023-08-23T20:51:50.409447Z","iopub.status.idle":"2023-08-23T20:51:51.322338Z","shell.execute_reply.started":"2023-08-23T20:51:50.409406Z","shell.execute_reply":"2023-08-23T20:51:51.321191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Joseph Brooks Directed the most number of movies ","metadata":{}},{"cell_type":"code","source":"#Plotting frequency of movies released by top 20 soundType\n\n# soundType data\nsoundType = transformed_df['soundType']\n\n# Count the occurrences of each soundType\nsoundType_counts = soundType.value_counts()\n\n# Select the top 20 soundType\ntop_20 = soundType_counts.head(20)\n\n# Plotting\nplt.figure(figsize=(14, 10))\ntop_20.plot(kind='bar', color='#90EE90')\nplt.title('Top 20 Sound Type by Number of Movies')\nplt.xlabel('sound Type')\nplt.ylabel('Number of Movies')\nplt.xticks(rotation=45, ha='right')\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:51:51.323818Z","iopub.execute_input":"2023-08-23T20:51:51.324157Z","iopub.status.idle":"2023-08-23T20:51:51.972703Z","shell.execute_reply.started":"2023-08-23T20:51:51.324128Z","shell.execute_reply":"2023-08-23T20:51:51.971549Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this graph we can see that Dolby is the most used sound type","metadata":{}},{"cell_type":"code","source":"# Applying SVD\nsvd = TruncatedSVD(n_components=10)\n\n#Train data\nsvd_result = svd.fit_transform(sparse_matrix) #reviewtext\nsvd_result1 = svd.transform(sparse_matrix1) \nsvd_genre_result = svd.fit_transform(sparse) #genre\nsvd_genre_result1 = svd.transform(sparse1) \nsvd_rc_result = svd.fit_transform(rc_sparse)   #ratingContents\nsvd_rc_result1 = svd.transform(rc_sparse1) ","metadata":{"id":"qVrUxCm_2dAE","execution":{"iopub.status.busy":"2023-08-23T20:51:51.974268Z","iopub.execute_input":"2023-08-23T20:51:51.974630Z","iopub.status.idle":"2023-08-23T20:51:56.424062Z","shell.execute_reply.started":"2023-08-23T20:51:51.974599Z","shell.execute_reply":"2023-08-23T20:51:56.423104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Conerting to DataFrame\n\n# Train data\ns_df = pd.DataFrame(svd_result)\nsg_df = pd.DataFrame(svd_genre_result)\nrc_df = pd.DataFrame(svd_rc_result)\n\n#Test data\ns_df1 = pd.DataFrame(svd_result1)\nsg_df1 = pd.DataFrame(svd_genre_result1)\nrc_df1 = pd.DataFrame(svd_rc_result1)","metadata":{"id":"aQ5RaB5s91g6","execution":{"iopub.status.busy":"2023-08-23T20:51:56.425292Z","iopub.execute_input":"2023-08-23T20:51:56.426038Z","iopub.status.idle":"2023-08-23T20:51:56.432709Z","shell.execute_reply.started":"2023-08-23T20:51:56.425996Z","shell.execute_reply":"2023-08-23T20:51:56.431572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_to_drop = ['reviewText','releaseDateTheaters','releaseDateStreaming','rating']\n\n# Drop the specified columns from the DataFrame of train dataset\np_df = transformed_df.drop(columns=columns_to_drop)\n# Drop the specified columns from the DataFrame of test dataset\np_df1 = transformed_df1.drop(columns=columns_to_drop)","metadata":{"id":"l69ggNSW-BM1","execution":{"iopub.status.busy":"2023-08-23T20:51:56.434294Z","iopub.execute_input":"2023-08-23T20:51:56.435103Z","iopub.status.idle":"2023-08-23T20:51:56.477813Z","shell.execute_reply.started":"2023-08-23T20:51:56.435065Z","shell.execute_reply":"2023-08-23T20:51:56.476742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:51:56.479480Z","iopub.execute_input":"2023-08-23T20:51:56.479927Z","iopub.status.idle":"2023-08-23T20:51:56.499190Z","shell.execute_reply.started":"2023-08-23T20:51:56.479890Z","shell.execute_reply":"2023-08-23T20:51:56.497965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p_df1.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:51:56.500484Z","iopub.execute_input":"2023-08-23T20:51:56.501413Z","iopub.status.idle":"2023-08-23T20:51:56.519477Z","shell.execute_reply.started":"2023-08-23T20:51:56.501372Z","shell.execute_reply":"2023-08-23T20:51:56.518356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_df = pd.concat([s_df, p_df, rc_df], axis=1)\ncombined_df1 = pd.concat([s_df1, p_df1, rc_df1], axis=1)","metadata":{"id":"E9FF83beKseJ","execution":{"iopub.status.busy":"2023-08-23T20:51:56.521086Z","iopub.execute_input":"2023-08-23T20:51:56.521542Z","iopub.status.idle":"2023-08-23T20:51:56.588899Z","shell.execute_reply.started":"2023-08-23T20:51:56.521475Z","shell.execute_reply":"2023-08-23T20:51:56.587843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:51:56.590208Z","iopub.execute_input":"2023-08-23T20:51:56.590555Z","iopub.status.idle":"2023-08-23T20:51:56.618322Z","shell.execute_reply.started":"2023-08-23T20:51:56.590525Z","shell.execute_reply":"2023-08-23T20:51:56.617253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"combined_df.columns = combined_df.columns.astype(str)\ncombined_df1.columns = combined_df.columns.astype(str)","metadata":{"id":"D3obuc2iLKKm","execution":{"iopub.status.busy":"2023-08-23T20:51:56.619770Z","iopub.execute_input":"2023-08-23T20:51:56.620110Z","iopub.status.idle":"2023-08-23T20:51:56.624984Z","shell.execute_reply.started":"2023-08-23T20:51:56.620082Z","shell.execute_reply":"2023-08-23T20:51:56.624080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cc = p_df.select_dtypes(include=['object', 'bool']).columns.tolist()\ncc1 = p_df1.select_dtypes(include=['object', 'bool']).columns.tolist()","metadata":{"id":"OO5j_NjIHH1x","execution":{"iopub.status.busy":"2023-08-23T20:51:56.626331Z","iopub.execute_input":"2023-08-23T20:51:56.626943Z","iopub.status.idle":"2023-08-23T20:51:56.688166Z","shell.execute_reply.started":"2023-08-23T20:51:56.626905Z","shell.execute_reply":"2023-08-23T20:51:56.686628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cc","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:51:56.689725Z","iopub.execute_input":"2023-08-23T20:51:56.690635Z","iopub.status.idle":"2023-08-23T20:51:56.697162Z","shell.execute_reply.started":"2023-08-23T20:51:56.690600Z","shell.execute_reply":"2023-08-23T20:51:56.696379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cc1","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:51:56.698474Z","iopub.execute_input":"2023-08-23T20:51:56.699051Z","iopub.status.idle":"2023-08-23T20:51:56.710645Z","shell.execute_reply.started":"2023-08-23T20:51:56.699019Z","shell.execute_reply":"2023-08-23T20:51:56.709515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Encoding categorical columns**","metadata":{}},{"cell_type":"code","source":"oridinal = OrdinalEncoder()\nohe= OneHotEncoder()\n\n# Use the ColumnTransformer to apply the appropriate transformation to each column\npreprocessort = ColumnTransformer(transformers=[\n    ('cat', oridinal , cc)\n   ],remainder='passthrough')\n\n# Fit and transform your data using the ColumnTransformer\nX_t = preprocessort.fit_transform(combined_df)\n\n# Use the ColumnTransformer to apply the appropriate transformation to each column\npreprocessort1 = ColumnTransformer(transformers=[\n    ('cat', oridinal , cc1)\n   ],remainder='passthrough')\n\n# Fit and transform your data using the ColumnTransformer\nX_t1 = preprocessort1.fit_transform(combined_df1)","metadata":{"id":"NuLY7UBuSFh2","outputId":"cfe5230a-dd44-4a67-be27-b44277340987","execution":{"iopub.status.busy":"2023-08-23T20:51:56.712603Z","iopub.execute_input":"2023-08-23T20:51:56.713287Z","iopub.status.idle":"2023-08-23T20:51:58.717475Z","shell.execute_reply.started":"2023-08-23T20:51:56.713249Z","shell.execute_reply":"2023-08-23T20:51:58.716331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_t.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:51:58.718912Z","iopub.execute_input":"2023-08-23T20:51:58.719270Z","iopub.status.idle":"2023-08-23T20:51:58.726213Z","shell.execute_reply.started":"2023-08-23T20:51:58.719240Z","shell.execute_reply":"2023-08-23T20:51:58.724968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_t1.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-23T20:51:58.727843Z","iopub.execute_input":"2023-08-23T20:51:58.728541Z","iopub.status.idle":"2023-08-23T20:51:58.739872Z","shell.execute_reply.started":"2023-08-23T20:51:58.728484Z","shell.execute_reply":"2023-08-23T20:51:58.738758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Train Test Split**","metadata":{"id":"uUcplrZuSFh6"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import RandomOverSampler # import RandomUnderSampler\noversampler = RandomOverSampler(random_state=22, sampling_strategy=1.0) # create an undersampler object with 1:1 ratio\n\n# Fit and transform the training data\nX1, y1 = oversampler.fit_resample(X_t, y_enc) # use undersampler instead of oversampler\n\nX_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.25, random_state=44, shuffle=True)","metadata":{"id":"bfpJ775f2dAE","execution":{"iopub.status.busy":"2023-08-23T20:51:58.741190Z","iopub.execute_input":"2023-08-23T20:51:58.741587Z","iopub.status.idle":"2023-08-23T20:51:59.259630Z","shell.execute_reply.started":"2023-08-23T20:51:58.741560Z","shell.execute_reply":"2023-08-23T20:51:59.258460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **ML Models**","metadata":{}},{"cell_type":"markdown","source":"**Model 1: LogisticRegression**","metadata":{"id":"yr6G4Y18SFiA"}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score,f1_score, confusion_matrix,roc_curve, roc_auc_score, precision_recall_curve, auc\nclf = LogisticRegression(random_state=22, penalty='l2', C=0.75 , solver= \"newton-cg\",max_iter=1000)\nclf.fit(X_train, y_train)\n# Make predictions on the test dataset\ny_pred = clf.predict(X_test)\n\n# Calculate the accuracy score\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy Score: {accuracy}\")\n\n# Calculate the F1 score\nf1 = f1_score(y_test, y_pred)\nprint(f\"F1 Score: {f1}\")\n\n\ny_pred_prob = clf.predict_proba(X_test)[:, 1]\n\n# Calculate ROC curve values\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n# Calculate the AUC score\nroc_auc = roc_auc_score(y_test, y_pred_prob)\n\n# Plot ROC curve\nplt.figure(figsize=(7, 6))\nplt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (ROC)')\nplt.legend(loc='lower right')\nplt.show()\n\n# Calculate precision-recall curve values\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n\n# Calculate the area under the precision-recall curve (AUC-PR)\npr_auc = auc(recall, precision)\n\n# Plot precision-recall curve\nplt.figure(figsize=(7, 6))\nplt.plot(recall, precision, color='blue', lw=2, label='PR curve (AUC-PR = %0.2f)' % pr_auc)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve')\nplt.legend(loc='lower left')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.show()\n\n# Calculate and plot the confusion matrix\nconfusion = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(7, 6))\nsns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\")\nplt.xlabel(\"Predicted Labels\")\nplt.ylabel(\"True Labels\")\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"id":"JYr6BjI42dAF","outputId":"45cf27fd-7e62-416f-ad94-76a09a2c7a42","execution":{"iopub.status.busy":"2023-08-23T20:51:59.261082Z","iopub.execute_input":"2023-08-23T20:51:59.261430Z","iopub.status.idle":"2023-08-23T20:52:42.431751Z","shell.execute_reply.started":"2023-08-23T20:51:59.261401Z","shell.execute_reply":"2023-08-23T20:52:42.430729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Accuracy Score: 0.6569390044316948\n* F1 Score: 0.6694074284094131","metadata":{"id":"-GVl19qCSFiD"}},{"cell_type":"markdown","source":"**Model 2: StackingClassifer (XGBoost)**","metadata":{}},{"cell_type":"code","source":"# import xgboost as xgb\n# from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier\n# from sklearn.ensemble import StackingClassifier\n# from sklearn.metrics import accuracy_score, f1_score\n\n# # Assuming you have X_train, X_test, y_train, y_test\n\n# # Create the base classifiers\n# xgb_classifier = xgb.XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=30, random_state=10)\n# ada_classifier = AdaBoostClassifier(n_estimators=100, random_state=10)\n# extra_trees_classifier = ExtraTreesClassifier(n_estimators=100, random_state=10)\n\n# # Create the StackingClassifier with XGBoost as the final_estimator\n# clf = StackingClassifier(\n#     estimators=[('xgb', xgb_classifier), ('ada', ada_classifier), ('et', extra_trees_classifier)],\n#     final_estimator=xgb_classifier\n# )\n\n# # Train the stacking classifier on the training data\n# clf.fit(X_train, y_train)\n# # Make predictions on the test dataset\n# y_pred = clf.predict(X_test)\n\n# # Calculate the accuracy score\n# accuracy = accuracy_score(y_test, y_pred)\n# print(f\"Accuracy Score: {accuracy}\")\n\n# # Calculate the F1 score\n# f1 = f1_score(y_test, y_pred)\n# print(f\"F1 Score: {f1}\")\n\n# y_pred_prob = clf.predict_proba(X_test)[:, 1]\n\n# # Calculate ROC curve values\n# fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n# # Calculate the AUC score\n# roc_auc = roc_auc_score(y_test, y_pred_prob)\n\n# # Plot ROC curve\n# plt.figure(figsize=(7, 6))\n# plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n# plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n# plt.xlim([0.0, 1.0])\n# plt.ylim([0.0, 1.05])\n# plt.xlabel('False Positive Rate')\n# plt.ylabel('True Positive Rate')\n# plt.title('Receiver Operating Characteristic (ROC)')\n# plt.legend(loc='lower right')\n# plt.show()\n\n# # Calculate precision-recall curve values\n# precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n\n# # Calculate the area under the precision-recall curve (AUC-PR)\n# pr_auc = auc(recall, precision)\n\n# # Plot precision-recall curve\n# plt.figure(figsize=(7, 6))\n# plt.plot(recall, precision, color='blue', lw=2, label='PR curve (AUC-PR = %0.2f)' % pr_auc)\n# plt.xlabel('Recall')\n# plt.ylabel('Precision')\n# plt.title('Precision-Recall Curve')\n# plt.legend(loc='lower left')\n# plt.xlim([0.0, 1.0])\n# plt.ylim([0.0, 1.05])\n# plt.show()\n\n# # Calculate and plot the confusion matrix\n# confusion = confusion_matrix(y_test, y_pred)\n# plt.figure(figsize=(7, 6))\n# sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\")\n# plt.xlabel(\"Predicted Labels\")\n# plt.ylabel(\"True Labels\")\n# plt.title(\"Confusion Matrix\")\n# plt.show()","metadata":{"id":"5gLLXpZrSFiE","execution":{"iopub.status.busy":"2023-08-23T20:52:42.433632Z","iopub.execute_input":"2023-08-23T20:52:42.434363Z","iopub.status.idle":"2023-08-23T20:52:42.442738Z","shell.execute_reply.started":"2023-08-23T20:52:42.434324Z","shell.execute_reply":"2023-08-23T20:52:42.441863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Accuracy Score: 0.8419116971000901\n* F1 Score: 0.8467202738602528","metadata":{"id":"wO9Mqv3cSFiI"}},{"cell_type":"markdown","source":"**Model 3: Stacking Classifer(Ensemble learning technique)**","metadata":{}},{"cell_type":"code","source":"# import pickle\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.svm import LinearSVC\n# from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier, StackingClassifier, RandomForestClassifier\n# import xgboost as xgb\n# from sklearn.metrics import accuracy_score, f1_score\n\n\n# # Define the base models\n# log_reg = LogisticRegression(penalty='l2', C=0.75, solver=\"newton-cg\")\n# xgb_classifier = xgb.XGBClassifier(learning_rate=0.1, n_estimators=100, max_depth=30, random_state=10)\n# ada_classifier = AdaBoosstClassifier(n_estimators=100, random_state=10)\n# extra_trees_classifier = ExtraTreesClassifier(n_estimators=100, random_state=10)\n# bagging_svm = BaggingClassifier(estimator=LinearSVC(C=0.001, loss='hinge', dual=True),  n_estimators=10, random_state=0)\n# random_forest_classifier = RandomForestClassifier(n_estimators=100, random_state=10)\n\n# # Define the stacking classifier\n\n# clf = StackingClassifier(\n#     estimators=[('lr', log_reg), ('xgb', xgb_classifier), ('ada', ada_classifier), ('et', extra_trees_classifier), ('bsvm', bagging_svm), ('rf', random_forest_classifier)],\n#     final_estimator=xgb_classifier\n# )\n\n# # Fit the stacking classifier on the train set\n# clf.fit(X_train, y_train)\n# # Save the stacked model using pickle\n# with open('stacked_model.pkl', 'wb') as model_file:\n#     pickle.dump(clf, model_file)\n\n# # Load the saved model using pickle\n# with open('stacked_model.pkl', 'rb') as model_file:\n#     loaded_model = pickle.load(model_file)\n\n# # Predict on the test set using the loaded model\n# y_pred = loaded_model.predict(X_test)\n\n# # Evaluate the accuracy and f1 scores\n# accuracy = accuracy_score(y_test, y_pred)\n# print(f\"Accuracy Score: {accuracy}\")\n# f1 = f1_score(y_test, y_pred)\n# print(f\"F1 Score: {f1}\")","metadata":{"id":"vP53yppxSFh8","execution":{"iopub.status.busy":"2023-08-23T20:52:42.443914Z","iopub.execute_input":"2023-08-23T20:52:42.444220Z","iopub.status.idle":"2023-08-23T20:52:42.462476Z","shell.execute_reply.started":"2023-08-23T20:52:42.444189Z","shell.execute_reply":"2023-08-23T20:52:42.461409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Accuracy Score: 0.8015483348963792\n* F1 Score: 0.8056895930860641","metadata":{}},{"cell_type":"markdown","source":"**Model 4: Lightbgm**","metadata":{}},{"cell_type":"code","source":"# import lightgbm as lgb\n# from sklearn.metrics import accuracy_score, f1_score\n\n# # Assuming you have X_train, X_test, y_train, y_test\n\n# # Create the LightGBM model\n# clf = lgb.LGBMClassifier(learning_rate= 0.2, max_depth= 7, min_child_samples= 10, n_estimators= 150, num_leaves=63)\n\n# # Train the model on the training data\n# clf.fit(X_train, y_train)\n\n# # Make predictions on the test dataset\n# y_pred = clf.predict(X_test)\n\n# # Calculate the accuracy score\n# accuracy = accuracy_score(y_test, y_pred)\n# print(f\"Accuracy Score: {accuracy}\")\n\n# # Calculate the F1 score\n# f1 = f1_score(y_test, y_pred)\n# print(f\"F1 Score: {f1}\")\n# y_pred_prob = clf.predict_proba(X_test)[:, 1]\n\n# # Calculate ROC curve values\n# fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n# # Calculate the AUC score\n# roc_auc = roc_auc_score(y_test, y_pred_prob)\n\n# # Plot ROC curve\n# plt.figure(figsize=(7, 6))\n# plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n# plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n# plt.xlim([0.0, 1.0])\n# plt.ylim([0.0, 1.05])\n# plt.xlabel('False Positive Rate')\n# plt.ylabel('True Positive Rate')\n# plt.title('Receiver Operating Characteristic (ROC)')\n# plt.legend(loc='lower right')\n# plt.show()\n\n# # Calculate precision-recall curve values\n# precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n\n# # Calculate the area under the precision-recall curve (AUC-PR)\n# pr_auc = auc(recall, precision)\n\n# # Plot precision-recall curve\n# plt.figure(figsize=(7, 6))\n# plt.plot(recall, precision, color='blue', lw=2, label='PR curve (AUC-PR = %0.2f)' % pr_auc)\n# plt.xlabel('Recall')\n# plt.ylabel('Precision')\n# plt.title('Precision-Recall Curve')\n# plt.legend(loc='lower left')\n# plt.xlim([0.0, 1.0])\n# plt.ylim([0.0, 1.05])\n# plt.show()\n\n# # Calculate and plot the confusion matrix\n# confusion = confusion_matrix(y_test, y_pred)\n# plt.figure(figsize=(7, 6))\n# sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\")\n# plt.xlabel(\"Predicted Labels\")\n# plt.ylabel(\"True Labels\")\n# plt.title(\"Confusion Matrix\")\n# plt.show()","metadata":{"id":"TrBiaq62SFiS","execution":{"iopub.status.busy":"2023-08-23T20:52:42.463673Z","iopub.execute_input":"2023-08-23T20:52:42.463967Z","iopub.status.idle":"2023-08-23T20:52:58.924440Z","shell.execute_reply.started":"2023-08-23T20:52:42.463942Z","shell.execute_reply":"2023-08-23T20:52:58.923286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Accuracy Score: 0.7376289512881338\n* F1 Score: 0.7344302572311359","metadata":{}},{"cell_type":"markdown","source":"**Model 5: LinearSVC**","metadata":{}},{"cell_type":"code","source":"# from sklearn.svm import LinearSVC\n\n# clf = LinearSVC(C=0.001, loss='squared_hinge', dual=True)\n# clf.fit(X_train, y_train)\n# y_pred = clf.predict(X_test)\n\n\n# # Evaluate the accuracy\n# accuracy = accuracy_score(y_test, y_pred)\n# print(\"Accuracy:\", accuracy)","metadata":{"id":"0SaEOTadPzHJ","execution":{"iopub.status.busy":"2023-08-23T20:52:58.925871Z","iopub.execute_input":"2023-08-23T20:52:58.926183Z","iopub.status.idle":"2023-08-23T20:52:58.931932Z","shell.execute_reply.started":"2023-08-23T20:52:58.926157Z","shell.execute_reply":"2023-08-23T20:52:58.930867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model 6: BaggingClassifer**","metadata":{}},{"cell_type":"code","source":"# from sklearn.svm import LinearSVC\n# from sklearn.ensemble import BaggingClassifier\n# from sklearn.metrics import accuracy_score\n\n# clf = BaggingClassifier(estimator=LinearSVC(C=1.2, loss='squared_hinge', dual=True),  n_estimators=10, random_state=0).fit(X_train, y_train)\n# clf.fit(X_train, y_train)\n\n# # Make predictions on the test dataset\n# y_pred = clf.predict(X_test)\n\n# # Calculate the accuracy score\n# accuracy = accuracy_score(y_test, y_pred)\n# print(f\"Accuracy Score: {accuracy}\")","metadata":{"id":"Nr9E1n7k2dAF","execution":{"iopub.status.busy":"2023-08-23T20:52:58.933158Z","iopub.execute_input":"2023-08-23T20:52:58.933455Z","iopub.status.idle":"2023-08-23T20:52:58.952666Z","shell.execute_reply.started":"2023-08-23T20:52:58.933429Z","shell.execute_reply":"2023-08-23T20:52:58.951662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model 7: KNeighborsClassifier**","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# from sklearn.neighbors import KNeighborsClassifier\n# from sklearn.metrics import accuracy_score\n\n\n# # Create a KNN classifier\n# clf = KNeighborsClassifier(n_neighbors=5)\n\n# # Fit the KNN classifier to the training data\n# clf.fit(X_train, y_train)\n\n# # Make predictions on the test data\n# y_pred = clf.predict(X_test)\n\n# # Evaluate the accuracy of the KNN classifier\n# accuracy = accuracy_score(y_test, y_pred)\n# print(\"Accuracy:\", accuracy)\n","metadata":{"id":"fIwgDA602dAF","execution":{"iopub.status.busy":"2023-08-23T20:52:58.954251Z","iopub.execute_input":"2023-08-23T20:52:58.954619Z","iopub.status.idle":"2023-08-23T20:52:58.963470Z","shell.execute_reply.started":"2023-08-23T20:52:58.954553Z","shell.execute_reply":"2023-08-23T20:52:58.962378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model 8: GradientBoostingClassifier**","metadata":{}},{"cell_type":"code","source":"# from sklearn.ensemble import GradientBoostingClassifier\n# from sklearn.metrics import accuracy_score\n\n# # Create and fit the GradientBoostingClassifier\n# clf = GradientBoostingClassifier(random_state=0, n_estimators=50,learning_rate= 0.01,max_depth= 3, min_samples_split= 2)\n# clf.fit(X_train, y_train)\n\n# # Make predictions on the test dataset\n# y_pred = clf.predict(X_test)\n\n# # Calculate the accuracy score\n# accuracy = accuracy_score(y_test, y_pred)\n# print(f\"Accuracy Score: {accuracy}\")\n","metadata":{"id":"f8-xqSeM2dAF","execution":{"iopub.status.busy":"2023-08-23T20:52:58.965205Z","iopub.execute_input":"2023-08-23T20:52:58.965585Z","iopub.status.idle":"2023-08-23T20:52:58.975158Z","shell.execute_reply.started":"2023-08-23T20:52:58.965556Z","shell.execute_reply":"2023-08-23T20:52:58.974238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model 9: MLPClassifier**","metadata":{}},{"cell_type":"code","source":"# from sklearn.neural_network import MLPClassifier\n# from sklearn.metrics import accuracy_score,f1_score\n\n# # Create the Multi-Layer Perceptron (MLP) classifier\n# clf = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam',random_state=0, verbose=True)\n\n# # Train the MLP classifier\n# clf.fit(X_train, y_train)\n\n# # Make predictions on the test dataset\n# y_pred = clf.predict(X_test)\n\n# # Calculate the accuracy score\n# accuracy = accuracy_score(y_test, y_pred)\n# print(f\"Accuracy Score: {accuracy}\")\n\n# # Calculate the F1 score\n# f1 = f1_score(y_test, y_pred)\n# print(f\"F1 Score: {f1}\")\n\n# y_pred_prob = clf.predict_proba(X_test)[:, 1]\n\n# # Calculate ROC curve values\n# fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n# # Calculate the AUC score\n# roc_auc = roc_auc_score(y_test, y_pred_prob)\n\n# # Plot ROC curve\n# plt.figure(figsize=(7, 6))\n# plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n# plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')\n# plt.xlim([0.0, 1.0])\n# plt.ylim([0.0, 1.05])\n# plt.xlabel('False Positive Rate')\n# plt.ylabel('True Positive Rate')\n# plt.title('Receiver Operating Characteristic (ROC)')\n# plt.legend(loc='lower right')\n# plt.show()\n\n# # Calculate precision-recall curve values\n# precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n\n# # Calculate the area under the precision-recall curve (AUC-PR)\n# pr_auc = auc(recall, precision)\n\n# # Plot precision-recall curve\n# plt.figure(figsize=(7, 6))\n# plt.plot(recall, precision, color='blue', lw=2, label='PR curve (AUC-PR = %0.2f)' % pr_auc)\n# plt.xlabel('Recall')\n# plt.ylabel('Precision')\n# plt.title('Precision-Recall Curve')\n# plt.legend(loc='lower left')\n# plt.xlim([0.0, 1.0])\n# plt.ylim([0.0, 1.05])\n# plt.show()\n\n# # Calculate and plot the confusion matrix\n# confusion = confusion_matrix(y_test, y_pred)\n# plt.figure(figsize=(7, 6))\n# sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\")\n# plt.xlabel(\"Predicted Labels\")\n# plt.ylabel(\"True Labels\")\n# plt.title(\"Confusion Matrix\")\n# plt.show()","metadata":{"id":"g-kMTvLG2dAG","execution":{"iopub.status.busy":"2023-08-23T20:52:58.976573Z","iopub.execute_input":"2023-08-23T20:52:58.976954Z","iopub.status.idle":"2023-08-23T20:52:58.986789Z","shell.execute_reply.started":"2023-08-23T20:52:58.976928Z","shell.execute_reply":"2023-08-23T20:52:58.985854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Accuracy Score: 0.5913246497911034\n* F1 Score: 0.6619915848527348","metadata":{}},{"cell_type":"markdown","source":"**Model 10: DecisionTreeClassifier**","metadata":{}},{"cell_type":"code","source":"# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.metrics import accuracy_score\n\n# clf = DecisionTreeClassifier(random_state=0)\n# clf.fit(X_train, y_train)\n# y_pred = clf.predict(X_test)\n# print(y_pred)\n\n# accuracy = accuracy_score(y_test, y_pred)\n# print(\"Accuracy:\", accuracy)\n","metadata":{"id":"9jP5OnGO2dAG","execution":{"iopub.status.busy":"2023-08-23T20:52:58.988047Z","iopub.execute_input":"2023-08-23T20:52:58.988361Z","iopub.status.idle":"2023-08-23T20:52:59.003825Z","shell.execute_reply.started":"2023-08-23T20:52:58.988335Z","shell.execute_reply":"2023-08-23T20:52:59.002592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Model 11: RandomForestClassifier**","metadata":{}},{"cell_type":"code","source":"# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.metrics import accuracy_score\n\n# # Assuming X_train, X_test, y_train, and y_test are defined and contain the appropriate data\n\n# clf = RandomForestClassifier(random_state=0)\n# clf.fit(X_train, y_train)\n# y_pred = clf.predict(X_test)\n# print(y_pred)\n\n# accuracy = accuracy_score(y_test, y_pred)\n# print(\"Accuracy:\", accuracy)\n","metadata":{"id":"2WMk9q_02dAG","execution":{"iopub.status.busy":"2023-08-23T20:52:59.005153Z","iopub.execute_input":"2023-08-23T20:52:59.005464Z","iopub.status.idle":"2023-08-23T20:52:59.019833Z","shell.execute_reply.started":"2023-08-23T20:52:59.005438Z","shell.execute_reply":"2023-08-23T20:52:59.018963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Submission**","metadata":{}},{"cell_type":"markdown","source":"**Classification Report**","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Assuming you have true labels y_true and predicted labels y_pred\nreport = classification_report(y_test, y_pred, zero_division=0)\n\n# Print the classification report\nprint(\"Classification Report:\")\nprint(report)","metadata":{"id":"BRYf5zt32dAH","execution":{"iopub.status.busy":"2023-08-23T20:52:59.021268Z","iopub.execute_input":"2023-08-23T20:52:59.021968Z","iopub.status.idle":"2023-08-23T20:52:59.172064Z","shell.execute_reply.started":"2023-08-23T20:52:59.021938Z","shell.execute_reply":"2023-08-23T20:52:59.171164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Predicting test data (y_pred) and decoding (y_pred)**","metadata":{}},{"cell_type":"code","source":"y_pred=clf.predict(X_t1)\nprint('y_pred:',y_pred)\ny_dec=encoder.inverse_transform(y_pred.reshape(-1, 1))","metadata":{"id":"bXMhW5Ho2dAH","execution":{"iopub.status.busy":"2023-08-23T20:52:59.183530Z","iopub.execute_input":"2023-08-23T20:52:59.184193Z","iopub.status.idle":"2023-08-23T20:52:59.443794Z","shell.execute_reply.started":"2023-08-23T20:52:59.184160Z","shell.execute_reply":"2023-08-23T20:52:59.442842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Submitting to competition**","metadata":{}},{"cell_type":"code","source":"submission=pd.DataFrame(columns=['id','sentiment'])\nsubmission['id']=[i for i in range(len(y_dec))]\nsubmission['sentiment']=y_dec\nsubmission.to_csv('submission1.csv',index=False)","metadata":{"id":"_KUhLSG5SFiw","execution":{"iopub.status.busy":"2023-08-23T20:52:59.445229Z","iopub.execute_input":"2023-08-23T20:52:59.445602Z","iopub.status.idle":"2023-08-23T20:52:59.671678Z","shell.execute_reply.started":"2023-08-23T20:52:59.445572Z","shell.execute_reply":"2023-08-23T20:52:59.670732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Hyperparameter Tuning**","metadata":{"id":"hWYwPBulSFiy"}},{"cell_type":"markdown","source":"**Model 1: Logistic Regression**","metadata":{"id":"zEJGGr1SSFiz"}},{"cell_type":"code","source":"# from sklearn.linear_model import LogisticRegression\n# from sklearn.metrics import accuracy_score\n# from sklearn.model_selection import GridSearchCV\n\n# # Assuming X_train, X_test, y_train, and y_test are defined and contain the appropriate data\n\n# # Create the Logistic Regression classifier\n# clf = LogisticRegression(random_state=42, max_iter=500)\n\n# # Define the hyperparameters to search\n# param_grid = {\n#     'C': [0.75, 0.5, 0.8],  # Try different regularization strengths\n#     'solver': ['newton-cg', 'lbfgs'],  # Try different solvers\n# }\n\n# # Initialize Grid Search with the classifier and the hyperparameter grid\n# grid_search = GridSearchCV(clf, param_grid, cv=5)\n\n# # Fit the Grid Search to the training data to find the best hyperparameters\n# grid_search.fit(X_train, y_train)\n\n# # Get the best estimator (model) from the grid search\n# best_model = grid_search.best_estimator_\n\n# # Use the best model to make predictions on the test set\n# y_pred = best_model.predict(X_test)\n\n# # Calculate accuracy\n# accuracy = accuracy_score(y_test, y_pred)\n# print(\"Best Hyperparameters:\", grid_search.best_params_)\n# print(\"Accuracy:\", accuracy)\n","metadata":{"id":"Jgcx3K5ySFi0","execution":{"iopub.status.busy":"2023-08-23T20:52:59.673016Z","iopub.execute_input":"2023-08-23T20:52:59.673776Z","iopub.status.idle":"2023-08-23T20:52:59.679779Z","shell.execute_reply.started":"2023-08-23T20:52:59.673746Z","shell.execute_reply":"2023-08-23T20:52:59.678612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Best Hyperparameters: {'C': 0.75, 'solver': 'newton-cg'}\n* Accuracy: 0.6468285763072518","metadata":{"id":"n6l43fOySFi2"}},{"cell_type":"markdown","source":"**Model 2: MLPClassifier**","metadata":{}},{"cell_type":"code","source":"# from sklearn.neural_network import MLPClassifier\n# from sklearn.metrics import accuracy_score\n# from sklearn.model_selection import GridSearchCV\n\n# # Assuming X_train, X_test, y_train, and y_test are defined and contain the appropriate data\n\n# # Create the Multi-Layer Perceptron (MLP) classifier\n# clf = MLPClassifier(random_state=0)\n\n# # Define the hyperparameters to search\n# param_grid = {\n#     'hidden_layer_sizes': [(50,), (100,), (150,)],  # Try different hidden layer sizes\n#     'activation': ['relu', 'tanh'],  # Try different activation functions\n#     'solver': ['adam', 'sgd'],  # Try different solvers\n#     'alpha': [0.0001, 0.001, 0.01],  # Try different regularization strengths\n# }\n\n# # Initialize Grid Search with the classifier and the hyperparameter grid\n# grid_search = GridSearchCV(clf, param_grid, cv=5)\n\n# # Fit the Grid Search to the training data to find the best hyperparameters\n# grid_search.fit(X_train, y_train)\n\n# # Get the best estimator (model) from the grid search\n# best_model = grid_search.best_estimator_\n\n# # Use the best model to make predictions on the test set\n# y_pred = best_model.predict(X_test)\n\n# # Calculate accuracy\n# accuracy = accuracy_score(y_test, y_pred)\n# print(\"Best Hyperparameters:\", grid_search.best_params_)\n# print(\"Accuracy:\", accuracy)\n\n# from sklearn.metrics import f1_score\n\n# # Assuming you have true labels y_true and predicted labels y_pred\n# f1_score = f1_score(y_test, y_pred,)\n\n# print(\"F1 Score:\", f1_score)","metadata":{"id":"Hs_jVsQ3SFi5","execution":{"iopub.status.busy":"2023-08-23T20:52:59.681054Z","iopub.execute_input":"2023-08-23T20:52:59.681371Z","iopub.status.idle":"2023-08-23T20:52:59.695383Z","shell.execute_reply.started":"2023-08-23T20:52:59.681344Z","shell.execute_reply":"2023-08-23T20:52:59.694413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Best Hyperparameters: {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'solver': 'adam'}\n* Accuracy: 0.6205456241051843\n* F1 Score: 0.645268237576591","metadata":{"id":"Uu7SYRp0SFi7"}},{"cell_type":"markdown","source":"**Model 3: LightGBM:**","metadata":{}},{"cell_type":"code","source":"# import lightgbm as lgb\n# from sklearn.metrics import accuracy_score\n# from sklearn.model_selection import GridSearchCV\n\n# # Assuming X_train, X_test, y_train, and y_test are defined and contain the appropriate data\n\n# # Create the LightGBM classifier\n# clf = lgb.LGBMClassifier(random_state=0)\n\n# # Define the hyperparameters to search\n# param_grid = {\n#     'n_estimators': [50, 100, 150],  # Try different numbers of trees\n#     'learning_rate': [0.01, 0.1, 0.2],  # Try different learning rates\n#     'max_depth': [3, 5, 7],  # Try different maximum depths\n#     'min_child_samples': [1, 5, 10],  # Try different minimum samples in a leaf node\n#     'num_leaves': [15, 31, 63],  # Try different numbers of leaves\n# }\n\n# # Initialize Grid Search with the classifier and the hyperparameter grid\n# grid_search = GridSearchCV(clf, param_grid, cv=5)\n\n# # Fit the Grid Search to the training data to find the best hyperparameters\n# grid_search.fit(X_train, y_train)\n\n# # Get the best estimator (model) from the grid search\n# best_model = grid_search.best_estimator_\n\n# # Use the best model to make predictions on the test set\n# y_pred = best_model.predict(X_test)\n\n# # Calculate accuracy\n# accuracy = accuracy_score(y_test, y_pred)\n# print(\"Best Hyperparameters:\", grid_search.best_params_)\n# print(\"Accuracy:\", accuracy)\n","metadata":{"id":"8aRqG_idSFi8","execution":{"iopub.status.busy":"2023-08-23T20:52:59.696726Z","iopub.execute_input":"2023-08-23T20:52:59.697092Z","iopub.status.idle":"2023-08-23T20:52:59.711136Z","shell.execute_reply.started":"2023-08-23T20:52:59.697063Z","shell.execute_reply":"2023-08-23T20:52:59.710141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Best Hyperparameters: {'learning_rate': 0.2, 'max_depth': 7, 'min_child_samples': 10, 'n_estimators': 150, 'num_leaves': 63}\n* Accuracy: 0.7214742621465068\n","metadata":{"id":"_QKhAIBBSFi_"}},{"cell_type":"code","source":"# from sklearn.naive_bayes import GaussianNB\n# from sklearn.metrics import accuracy_score, f1_score\n# from sklearn.model_selection import GridSearchCV\n\n# # Assuming X_train, X_test, y_train, and y_test are defined and contain the appropriate data\n\n# # Create the Gaussian Naive Bayes classifier\n# clf = GaussianNB()\n\n# # Define the hyperparameters to search\n# param_grid = {\n#     'var_smoothing': [1e-09, 1e-08, 1e-07, 1e-06, 1e-05]  # Try different values for var_smoothing\n# }\n\n# # Initialize Grid Search with the classifier and the hyperparameter grid\n# grid_search = GridSearchCV(clf, param_grid, cv=5)\n\n# # Fit the Grid Search to the training data to find the best hyperparameters\n# grid_search.fit(X_train, y_train)\n\n# # Get the best estimator (model) from the grid search\n# best_model = grid_search.best_estimator_\n\n# # Use the best model to make predictions on the test set\n# y_pred = best_model.predict(X_test)\n\n# # Calculate accuracy\n# accuracy = accuracy_score(y_test, y_pred)\n# print(\"Best Hyperparameters:\", grid_search.best_params_)\n# print(\"Accuracy:\", accuracy)\n\n# # Calculate the F1 score\n# f1 = f1_score(y_test, y_pred)\n# print(\"F1 Score:\", f1)\n","metadata":{"id":"CATcu6kXSFjA","execution":{"iopub.status.busy":"2023-08-23T20:52:59.712348Z","iopub.execute_input":"2023-08-23T20:52:59.712687Z","iopub.status.idle":"2023-08-23T20:52:59.726755Z","shell.execute_reply.started":"2023-08-23T20:52:59.712658Z","shell.execute_reply":"2023-08-23T20:52:59.725633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.metrics import classification_report\n# # Assuming you have true labels y_true and predicted labels y_pred\n# report = classification_report(y_test, y_pred, zero_division=0)\n# # Print the classification report\n# print(\"Classification Report:\")\n# print(report)\n# y_pre=clf.predict(X_t1)\n# print(y_pre)\n# y_dec=encoder.inverse_transform(y_pre.reshape(-1, 1))\n# print(y_dec)\n# submission = pd.DataFrame(columns=['id', 'sentiment'])\n# submission['id'] = [i for i in range(len(y_dec))]\n# submission['sentiment'] = y_dec\n# submission.to_csv('submission1.csv', index=False)","metadata":{"id":"F3q0Ar0oSFjC","execution":{"iopub.status.busy":"2023-08-23T20:52:59.728408Z","iopub.execute_input":"2023-08-23T20:52:59.728812Z","iopub.status.idle":"2023-08-23T20:52:59.743205Z","shell.execute_reply.started":"2023-08-23T20:52:59.728782Z","shell.execute_reply":"2023-08-23T20:52:59.741822Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Final note**","metadata":{}},{"cell_type":"markdown","source":"> **Data preprocessing**\n\nThere were three datasets given train, test, and movies. There were duplicate enitiries in the movies dataset, I droped the duplicates just by keeping the first one. Then merged the train with movies and test also with movies dataset. After that I have done the data preprocessing, the data had so much missing values. I have imputed some columns using replace functions. For the rest I used numerical and categorical transformed prior to using transformers on them I have splited them respectively. Then column Transformer was applied to the merged_df (train) and merged_df1 (test) they were transformed using pipeline function and they were named as transformed_df (train) and transformed_df1 (test) after apply fit_transform and transform fuctions respectively. The reviewText column was imputed sepreatly using the some columns in transformed_df and tranformed_df1 for train and test respectively. I have also applied tfidf on all columns containing text data to make a heat map. I encoded the categorical variables just before train_test_split using oridinal encoder because one hot encoder was giving column mismatch in train and test datasets.\n\n\n\n> **Model 1: Logistic regression (Best model)**\n \nIt is a powerful machine learning technique that can help you solve binary classification problems. It is based on the idea of using a mathematical function called the logistic function to model the probability of an outcome given some input features. Logistic regression can be used to predict whether an email is spam or not, whether a customer will buy a product or not, whether a tumor is malignant or benign, and many other applications.\n\nI have used the sklearn library to import the model and some evaluation metrics. I have also set some parameters for the model, such as the random_state, penalty, C, solver, and max_iter. You have then trained the model on the training data and made predictions on the test data. Finally, you have calculated the accuracy score and the F1 score to measure the performance of the model.\n\nMy model has achieved an accuracy score of 0.6556 and an F1 score of 0.6690. This is my best model!!!\nSubmission score: 0.66846\n\n\n>  **Model 2: StackingClassifer (XGBoost)**\n\nThe code you have written is for a stacking classifier, which is a type of ensemble learning technique that can combine multiple base classifiers to create a more powerful meta-classifier. I have used the xgboost, sklearn, and numpy libraries to import the XGBClassifier, AdaBoostClassifier, ExtraTreesClassifier, StackingClassifier, accuracy_score, and f1_score classes and functions. I have also specified some hyperparameters for the base classifiers, such as the learning_rate, n_estimators, max_depth, and random_state. You have then created the stacking classifier with XGBoost as the final_estimator, which means that it will use the predictions of the base classifiers as input features and make the final prediction.\n\nI have then trained the stacking classifier on the training data (X_train, y_train) and made predictions on the test data (X_test). You have also calculated the accuracy score and the F1 score to measure the performance of the model. The accuracy score is a metric that measures how well the model can correctly classify the test data.\n\n* Accuracy Score: 0.8505360328055754\n* F1 Score: 0.8570021111893033\n* Submission score: 0.65763\n\n> **Model 3: Stacking Classifer(Ensemble learning technique)**\n\nThe code you have written is for a stacking classifier, which is a type of ensemble learning technique that can combine multiple base classifiers to create a more powerful meta-classifier. You have used the xgboost, sklearn, and pickle libraries to import the XGBClassifier, LogisticRegression, LinearSVC, AdaBoostClassifier, ExtraTreesClassifier, BaggingClassifier, RandomForestClassifier, StackingClassifier, accuracy_score, f1_score classes and functions. You have also specified some hyperparameters for the base classifiers, such as the penalty, C, solver, learning_rate, n_estimators, max_depth, and random_state. I have then created the stacking classifier with six base classifiers (logistic regression, XGBoost, AdaBoost, extra trees, bagging SVM, and random forest) and XGBoost as the final_estimator, which means that it will use the predictions of the base classifiers as input features and make the final prediction.\n\nI have then trained the stacking classifier on the training data (X_train, y_train) and saved the model using pickle. Pickle is a module that can serialize and deserialize Python objects into binary files. You have used pickle to dump the model into a file named 'stacked_model.pkl'. I have then loaded the model from the file using pickle.load and made predictions on the test data (X_test). I have also calculated the accuracy score and the F1 score to measure the performance of the model.\n\n* Accuracy Score: 0.8015483348963792\n* F1 Score: 0.8056895930860641\n* Submission Score: 0.6359\n\n> **Model 4: LightGBM (Second Best Model):**\n\nThe code I have written is for a LightGBM model, which is a type of gradient boosting framework that can be used for classification, regression, or ranking problems. I have used the lightgbm and sklearn libraries to import the LGBMClassifier, accuracy_score, and f1_score classes and functions. I have also specified some hyperparameters for the model, such as the learning_rate, max_depth, min_child_samples, n_estimators, and num_leaves. I have then trained the model on the training data (X_train, y_train) and made predictions on the test data (X_test). Finally, I have evaluated the performance of the model by calculating the accuracy score and the F1 score.\n* Accuracy Score: 0.7376289512881338\n* F1 Score: 0.7344302572311359\n* Submission score: 0.66204\n\n> **Model 5: LinearSVC:**\n\nThe code I have written is for a linear support vector machine (SVM) model, which is a type of supervised learning algorithm that can be used for classification or regression problems. I have used the sklearn library to import the LinearSVC class and the accuracy_score function. I have also specified some hyperparameters for the model, such as the C, loss, and dual. You have then trained the model on the training data (X_train, y_train) and made predictions on the test data (X_test). Finally, I have evaluated the performance of the model by calculating the accuracy score.\n\n> **Model 6: BaggingClassifier (integrated with LinearSVC)**\n\nThe code I have written is for a bagging classifier, which is a type of ensemble learning technique that can create multiple bootstrap samples from the original data and train a base classifier on each sample. I have used the sklearn library to import the BaggingClassifier, LinearSVC, and accuracy_score classes and functions. I have also specified some hyperparameters for the base classifier and the bagging classifier, such as the C, loss, dual, n_estimators, and random_state. I have then trained the bagging classifier on the training data (X_train, y_train) using a linear support vector machine (SVM) as the base classifier. I have then made predictions on the test data (X_test) and calculated the accuracy score.\n\n> **Model 7: KNeighborsClassifier**\n\nThe code I have written is for a k-nearest neighbors (KNN) classifier, which is a type of supervised learning algorithm that can be used for classification or regression problems. I have used the pandas and sklearn libraries to import the KNeighborsClassifier and accuracy_score classes and functions. I have also specified the number of neighbors to be used in the KNN algorithm, which is 5. I have then trained the KNN classifier on the training data (X_train, y_train) and made predictions on the test data (X_test). Finally, I have evaluated the performance of the model by calculating the accuracy score.\n\n\n> **Model 8: GradientBoostingClassifier**\n\nThe code I have written is for a gradient boosting classifier, which is a type of ensemble learning technique that can create a strong learner by combining multiple weak learners. I have used the sklearn library to import the GradientBoostingClassifier and accuracy_score classes and functions. I have also specified some hyperparameters for the model, I as the random_state, n_estimators, learning_rate, max_depth, and min_samples_split. I have then trained the model on the training data (X_train, y_train) and made predictions on the test data (X_test). Finally, I have evaluated the performance of the model by calculating the accuracy score.\n\n> **Model 9: MLPClassifier (Best Model)**\n\nThe code I have written is for a multi-layer perceptron (MLP) classifier, which is a type of artificial neural network that can be used for classification or regression problems. I have used the sklearn and numpy libraries to import the MLPClassifier, accuracy_score, and f1_score classes and functions. I have also specified some hyperparameters for the model, such as the hidden_layer_sizes, activation, solver, random_state, and verbose. I have then trained the model on the training data (X_train, y_train) and made predictions on the test data (X_test). Finally, I have evaluated the performance of the model by calculating the accuracy score and the F1 score.\n* Accuracy Score: 0.5913246497911034\n* F1 Score: 0.6619915848527348\n* Submission score: 0.66846\n\n> **Model 10: DecisionTreeClassifier:**\n\nThe code I have written is for a decision tree classifier, which is a type of supervised learning algorithm that can be used for classification or regression problems. I have used the sklearn library to import the DecisionTreeClassifier and accuracy_score classes and functions. I have also specified the random_state parameter, which is a parameter that sets the seed for random number generation. This can help ensure reproducibility and consistency of results. I have then trained the model on the training data (X_train, y_train) and made predictions on the test data (X_test).\n\n> **Model 11: RandomForestClassifier:**\n\nThe code I have written is for a random forest classifier, which is a type of ensemble learning technique that can create a large number of decision trees and average their predictions. I have used the sklearn library to import the RandomForestClassifier and accuracy_score classes and functions. I have also specified the random_state parameter, which is a parameter that sets the seed for random number generation. This can help ensure reproducibility and consistency of results. I have then trained the model on the training data (X_train, y_train) and made predictions on the test data (X_test). Finally, I have printed the predictions and calculated the accuracy score.\n\n> **Inference**\n\nThe sentiment analysis using sklearn was a challenging task for me. In this project I have applied various feature engineering techniques to improve the model but I couldn't improve my submission score beyond .66846. The best models which cleared the cutoff scores are LogisticRegression, MLPClassifier and LightGBM.\n\n","metadata":{}}]}